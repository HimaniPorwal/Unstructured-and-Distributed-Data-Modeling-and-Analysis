{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sacred-powell",
   "metadata": {},
   "source": [
    "## MSBX 5420 Assignment 3\n",
    "This assignment is about Spark Machine Learning and Spark Streaming. First two tasks focus on machine learning, and the third one combines machine learning and streaming analysis. We will use IMDB reviews data for the whole assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-denmark",
   "metadata": {},
   "source": [
    "### Task 1 - Topic Modeling on Moive Reviews with Spark ML\n",
    "First of all, let's load the data. The data structure is very simple.One column is review text, and another column is the label of review sentiment (positive or negative). Same as exercise, we can load .csv.gz file directly from spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "absent-embassy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master('local[4]').config(\"spark.driver.memory\", \"2g\").appName('spark_ml_imdb').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "familiar-fashion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|              review|sentiment|\n",
      "+--------------------+---------+\n",
      "|One of the other ...| positive|\n",
      "|A wonderful littl...| positive|\n",
      "|I thought this wa...| positive|\n",
      "|Basically there's...| negative|\n",
      "|Petter Mattei's \"...| positive|\n",
      "|Probably my all-t...| positive|\n",
      "|I sure would like...| positive|\n",
      "|This show was an ...| negative|\n",
      "|Encouraged by the...| negative|\n",
      "|If you like origi...| positive|\n",
      "|Phil the Alien is...| negative|\n",
      "|I saw this movie ...| negative|\n",
      "|So im not a big f...| negative|\n",
      "|The cast played S...| negative|\n",
      "|This a fantastic ...| positive|\n",
      "|Kind of drawn in ...| negative|\n",
      "|Some films just s...| positive|\n",
      "|This movie made i...| negative|\n",
      "|I remember this f...| positive|\n",
      "|An awful film! It...| negative|\n",
      "+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews = spark.read.options(inferSchema = True, multiLine = True, escape = '\\\"').csv('IMDB_Reviews.csv.gz', header=True)\n",
    "reviews.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eight-liver",
   "metadata": {},
   "source": [
    "First, we should clean up the review texts. Besides those special characters we have tried to remove in exercise, here we also need to remove the html tags in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "oriented-insert",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(review='One of the other reviewers has mentioned that after watching just 1 Oz episode you ll be hooked  They are right  as this is exactly what happened with me   The first thing that struck me about Oz was its brutality and unflinching scenes of violence  which set in right from the word GO  Trust me  this is not a show for the faint hearted or timid  This show pulls no punches with regards to drugs  sex or violence  Its is hardcore  in the classic use of the word   It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary  It focuses mainly on Emerald City  an experimental section of the prison where all the cells have glass fronts and face inwards  so privacy is not high on the agenda  Em City is home to many Aryans  Muslims  gangstas  Latinos  Christians  Italians  Irish and more so scuffles  death stares  dodgy dealings and shady agreements are never far away   I would say the main appeal of the show is due to the fact that it goes where other shows wouldn t dare  Forget pretty pictures painted for mainstream audiences  forget charm  forget romance OZ doesn t mess around  The first episode I ever saw struck me as so nasty it was surreal  I couldn t say I was ready for it  but as I watched more  I developed a taste for Oz  and got accustomed to the high levels of graphic violence  Not just violence  but injustice  crooked guards who ll be sold out for a nickel  inmates who ll kill on order and get away with it  well mannered  middle class inmates being turned into prison bitches due to their lack of street skills or prison experience  Watching Oz  you may become comfortable with what is uncomfortable viewing thats if you can get in touch with your darker side ', sentiment='positive')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.sql.functions as fn\n",
    "import pyspark.ml.feature as ft\n",
    "\n",
    "#remove html tags in the text with regular expression\n",
    "reviews = reviews.withColumn('review', fn.regexp_replace(fn.col(\"review\"), '<[^>]+>', ' '))\n",
    "#remove special characters and line breaks in the text with regular expression\n",
    "reviews = reviews.withColumn('review', fn.regexp_replace(fn.col(\"review\"), '([^\\s\\w_]|_)+', ' ')).withColumn('review', fn.regexp_replace(fn.col(\"review\"), '[\\n\\r]', ' '))\n",
    "reviews.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-individual",
   "metadata": {},
   "source": [
    "Now let's create tokenizer to start the data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "circular-excellence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(review_tok=['one', 'of', 'the', 'other', 'reviewers', 'has', 'mentioned', 'that', 'after', 'watching', 'just', '1', 'oz', 'episode', 'you', 'll', 'be', 'hooked', 'they', 'are', 'right', 'as', 'this', 'is', 'exactly', 'what', 'happened', 'with', 'me', 'the', 'first', 'thing', 'that', 'struck', 'me', 'about', 'oz', 'was', 'its', 'brutality', 'and', 'unflinching', 'scenes', 'of', 'violence', 'which', 'set', 'in', 'right', 'from', 'the', 'word', 'go', 'trust', 'me', 'this', 'is', 'not', 'a', 'show', 'for', 'the', 'faint', 'hearted', 'or', 'timid', 'this', 'show', 'pulls', 'no', 'punches', 'with', 'regards', 'to', 'drugs', 'sex', 'or', 'violence', 'its', 'is', 'hardcore', 'in', 'the', 'classic', 'use', 'of', 'the', 'word', 'it', 'is', 'called', 'oz', 'as', 'that', 'is', 'the', 'nickname', 'given', 'to', 'the', 'oswald', 'maximum', 'security', 'state', 'penitentary', 'it', 'focuses', 'mainly', 'on', 'emerald', 'city', 'an', 'experimental', 'section', 'of', 'the', 'prison', 'where', 'all', 'the', 'cells', 'have', 'glass', 'fronts', 'and', 'face', 'inwards', 'so', 'privacy', 'is', 'not', 'high', 'on', 'the', 'agenda', 'em', 'city', 'is', 'home', 'to', 'many', 'aryans', 'muslims', 'gangstas', 'latinos', 'christians', 'italians', 'irish', 'and', 'more', 'so', 'scuffles', 'death', 'stares', 'dodgy', 'dealings', 'and', 'shady', 'agreements', 'are', 'never', 'far', 'away', 'i', 'would', 'say', 'the', 'main', 'appeal', 'of', 'the', 'show', 'is', 'due', 'to', 'the', 'fact', 'that', 'it', 'goes', 'where', 'other', 'shows', 'wouldn', 't', 'dare', 'forget', 'pretty', 'pictures', 'painted', 'for', 'mainstream', 'audiences', 'forget', 'charm', 'forget', 'romance', 'oz', 'doesn', 't', 'mess', 'around', 'the', 'first', 'episode', 'i', 'ever', 'saw', 'struck', 'me', 'as', 'so', 'nasty', 'it', 'was', 'surreal', 'i', 'couldn', 't', 'say', 'i', 'was', 'ready', 'for', 'it', 'but', 'as', 'i', 'watched', 'more', 'i', 'developed', 'a', 'taste', 'for', 'oz', 'and', 'got', 'accustomed', 'to', 'the', 'high', 'levels', 'of', 'graphic', 'violence', 'not', 'just', 'violence', 'but', 'injustice', 'crooked', 'guards', 'who', 'll', 'be', 'sold', 'out', 'for', 'a', 'nickel', 'inmates', 'who', 'll', 'kill', 'on', 'order', 'and', 'get', 'away', 'with', 'it', 'well', 'mannered', 'middle', 'class', 'inmates', 'being', 'turned', 'into', 'prison', 'bitches', 'due', 'to', 'their', 'lack', 'of', 'street', 'skills', 'or', 'prison', 'experience', 'watching', 'oz', 'you', 'may', 'become', 'comfortable', 'with', 'what', 'is', 'uncomfortable', 'viewing', 'thats', 'if', 'you', 'can', 'get', 'in', 'touch', 'with', 'your', 'darker', 'side'])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = ft.RegexTokenizer(inputCol='review', outputCol='review_tok', pattern='\\s+|[,.\\\"/!]')\n",
    "tokenizer.transform(reviews).select('review_tok').take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waiting-moses",
   "metadata": {},
   "source": [
    "Then remove stopwords in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "engaging-kingston",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(review_stop=['one', 'reviewers', 'mentioned', 'watching', '1', 'oz', 'episode', 'll', 'hooked', 'right', 'exactly', 'happened', 'first', 'thing', 'struck', 'oz', 'brutality', 'unflinching', 'scenes', 'violence', 'set', 'right', 'word', 'go', 'trust', 'show', 'faint', 'hearted', 'timid', 'show', 'pulls', 'punches', 'regards', 'drugs', 'sex', 'violence', 'hardcore', 'classic', 'use', 'word', 'called', 'oz', 'nickname', 'given', 'oswald', 'maximum', 'security', 'state', 'penitentary', 'focuses', 'mainly', 'emerald', 'city', 'experimental', 'section', 'prison', 'cells', 'glass', 'fronts', 'face', 'inwards', 'privacy', 'high', 'agenda', 'em', 'city', 'home', 'many', 'aryans', 'muslims', 'gangstas', 'latinos', 'christians', 'italians', 'irish', 'scuffles', 'death', 'stares', 'dodgy', 'dealings', 'shady', 'agreements', 'never', 'far', 'away', 'say', 'main', 'appeal', 'show', 'due', 'fact', 'goes', 'shows', 'wouldn', 'dare', 'forget', 'pretty', 'pictures', 'painted', 'mainstream', 'audiences', 'forget', 'charm', 'forget', 'romance', 'oz', 'doesn', 'mess', 'around', 'first', 'episode', 'ever', 'saw', 'struck', 'nasty', 'surreal', 'couldn', 'say', 'ready', 'watched', 'developed', 'taste', 'oz', 'got', 'accustomed', 'high', 'levels', 'graphic', 'violence', 'violence', 'injustice', 'crooked', 'guards', 'll', 'sold', 'nickel', 'inmates', 'll', 'kill', 'order', 'get', 'away', 'well', 'mannered', 'middle', 'class', 'inmates', 'turned', 'prison', 'bitches', 'due', 'lack', 'street', 'skills', 'prison', 'experience', 'watching', 'oz', 'may', 'become', 'comfortable', 'uncomfortable', 'viewing', 'thats', 'get', 'touch', 'darker', 'side'])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = ft.StopWordsRemover(inputCol=tokenizer.getOutputCol(), outputCol='review_stop')\n",
    "stopwords.transform(tokenizer.transform(reviews)).select('review_stop').take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-washer",
   "metadata": {},
   "source": [
    "Now same as what we did in the exercise, let's create `CountVectorizer` to transform the text into term frequency vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "finite-research",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(review_tf=SparseVector(101111, {2: 1.0, 10: 1.0, 13: 2.0, 17: 2.0, 28: 1.0, 32: 1.0, 35: 3.0, 39: 1.0, 45: 2.0, 46: 1.0, 50: 1.0, 53: 1.0, 54: 2.0, 57: 1.0, 83: 1.0, 85: 1.0, 91: 1.0, 93: 1.0, 97: 1.0, 101: 2.0, 108: 1.0, 121: 1.0, 128: 3.0, 138: 2.0, 160: 1.0, 161: 1.0, 169: 1.0, 174: 1.0, 184: 1.0, 191: 2.0, 195: 1.0, 217: 1.0, 235: 1.0, 249: 1.0, 250: 1.0, 251: 1.0, 264: 1.0, 278: 1.0, 286: 2.0, 302: 1.0, 316: 1.0, 324: 1.0, 370: 1.0, 386: 1.0, 409: 2.0, 438: 1.0, 448: 4.0, 453: 1.0, 457: 1.0, 480: 1.0, 501: 1.0, 514: 1.0, 526: 1.0, 534: 2.0, 535: 1.0, 567: 2.0, 582: 1.0, 685: 1.0, 707: 3.0, 754: 1.0, 779: 1.0, 826: 1.0, 921: 1.0, 939: 1.0, 1073: 3.0, 1090: 1.0, 1107: 1.0, 1146: 1.0, 1181: 1.0, 1210: 1.0, 1290: 1.0, 1293: 1.0, 1313: 1.0, 1347: 1.0, 1464: 1.0, 1473: 1.0, 1496: 1.0, 1619: 1.0, 1899: 1.0, 1916: 1.0, 1936: 1.0, 2015: 1.0, 2117: 1.0, 2211: 1.0, 2328: 1.0, 2340: 1.0, 2381: 1.0, 2421: 1.0, 2471: 1.0, 2588: 1.0, 2790: 1.0, 2792: 1.0, 2865: 1.0, 2903: 1.0, 2967: 6.0, 3093: 1.0, 3132: 2.0, 3209: 1.0, 3709: 1.0, 3733: 1.0, 3993: 1.0, 4051: 1.0, 4577: 1.0, 4781: 1.0, 4909: 1.0, 4939: 1.0, 5282: 1.0, 5416: 1.0, 5824: 1.0, 6761: 1.0, 6784: 2.0, 6917: 1.0, 7087: 1.0, 7180: 1.0, 7512: 1.0, 7596: 1.0, 7743: 1.0, 7838: 1.0, 8093: 1.0, 8654: 1.0, 9087: 1.0, 10096: 1.0, 11408: 1.0, 11636: 1.0, 11954: 1.0, 12358: 1.0, 14710: 1.0, 14903: 1.0, 15158: 1.0, 16647: 1.0, 19312: 1.0, 22835: 1.0, 22840: 1.0, 24934: 1.0, 32519: 1.0, 41841: 1.0, 47495: 1.0, 54399: 1.0, 56386: 1.0}))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = ft.CountVectorizer(inputCol=stopwords.getOutputCol(), outputCol='review_tf')\n",
    "tokenized = stopwords.transform(tokenizer.transform(reviews))\n",
    "tf.fit(tokenized).transform(tokenized).select('review_tf').take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opening-mercy",
   "metadata": {},
   "source": [
    "Then we use the `LDA` model to do topic modeling. We create the model here with 30 topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "received-spyware",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.clustering as clus\n",
    "lda = clus.LDA(k=30, optimizer='online', maxIter=10, featuresCol=tf.getOutputCol())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-princess",
   "metadata": {},
   "source": [
    "Now let's build the pipeline to train the topic model from the raw data. It will take a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fa50c6d-dfef-4433-b2ba-16c61997b1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Define stages of the pipeline\n",
    "pipeline_stages = [\n",
    "    # Tokenize the review text\n",
    "    tokenizer,\n",
    "    # Remove stopwords\n",
    "    stopwords,\n",
    "    # Convert text into a sparse vector of token counts\n",
    "    tf,\n",
    "    # Fit LDA model\n",
    "    lda\n",
    "]\n",
    "\n",
    "# Create the pipeline\n",
    "lda_pipeline = Pipeline(stages=pipeline_stages)\n",
    "\n",
    "# Fit the pipeline to the data\n",
    "pipeline_model = lda_pipeline.fit(reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "faa7c5fb-a254-461d-94ed-1115ae1ec6a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(topicDistribution=DenseVector([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.9946])),\n",
       " Row(topicDistribution=DenseVector([0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.9894])),\n",
       " Row(topicDistribution=DenseVector([0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.9895])),\n",
       " Row(topicDistribution=DenseVector([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.9865])),\n",
       " Row(topicDistribution=DenseVector([0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.9927]))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics = pipeline_model.transform(reviews)\n",
    "topics.select('topicDistribution').take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-identity",
   "metadata": {},
   "source": [
    "Let's see if we have properly discovered the topics. This is just the same code we display topics in the exercise - we will reuse it several times here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "metropolitan-crest",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic: 0\n",
      "*************************\n",
      "film\n",
      "movie\n",
      "one\n",
      "time\n",
      "made\n",
      "good\n",
      "like\n",
      "bad\n",
      "films\n",
      "get\n",
      "two\n",
      "always\n",
      "first\n",
      "plot\n",
      "father\n",
      "little\n",
      "years\n",
      "also\n",
      "cast\n",
      "life\n",
      "*************************\n",
      "topic: 1\n",
      "*************************\n",
      "movie\n",
      "bad\n",
      "part\n",
      "film\n",
      "one\n",
      "like\n",
      "us\n",
      "still\n",
      "time\n",
      "character\n",
      "story\n",
      "good\n",
      "much\n",
      "two\n",
      "even\n",
      "horizon\n",
      "end\n",
      "make\n",
      "lost\n",
      "fuller\n",
      "*************************\n",
      "topic: 2\n",
      "*************************\n",
      "film\n",
      "one\n",
      "movie\n",
      "good\n",
      "like\n",
      "well\n",
      "great\n",
      "never\n",
      "show\n",
      "first\n",
      "really\n",
      "story\n",
      "also\n",
      "see\n",
      "made\n",
      "much\n",
      "get\n",
      "watch\n",
      "time\n",
      "two\n",
      "*************************\n",
      "topic: 3\n",
      "*************************\n",
      "movie\n",
      "one\n",
      "film\n",
      "series\n",
      "first\n",
      "story\n",
      "movies\n",
      "good\n",
      "really\n",
      "like\n",
      "much\n",
      "watch\n",
      "little\n",
      "puppet\n",
      "get\n",
      "also\n",
      "even\n",
      "time\n",
      "book\n",
      "made\n",
      "*************************\n",
      "topic: 4\n",
      "*************************\n",
      "movie\n",
      "one\n",
      "like\n",
      "film\n",
      "movies\n",
      "great\n",
      "bad\n",
      "also\n",
      "really\n",
      "story\n",
      "good\n",
      "get\n",
      "made\n",
      "much\n",
      "even\n",
      "scene\n",
      "well\n",
      "see\n",
      "think\n",
      "characters\n",
      "*************************\n",
      "topic: 5\n",
      "*************************\n",
      "movie\n",
      "film\n",
      "one\n",
      "like\n",
      "really\n",
      "good\n",
      "even\n",
      "many\n",
      "movies\n",
      "see\n",
      "time\n",
      "well\n",
      "great\n",
      "films\n",
      "make\n",
      "story\n",
      "made\n",
      "first\n",
      "character\n",
      "plot\n",
      "*************************\n",
      "topic: 6\n",
      "*************************\n",
      "movie\n",
      "film\n",
      "good\n",
      "one\n",
      "like\n",
      "time\n",
      "even\n",
      "first\n",
      "also\n",
      "well\n",
      "made\n",
      "scene\n",
      "get\n",
      "much\n",
      "never\n",
      "people\n",
      "old\n",
      "plot\n",
      "story\n",
      "jane\n",
      "*************************\n",
      "topic: 7\n",
      "*************************\n",
      "movie\n",
      "one\n",
      "good\n",
      "film\n",
      "show\n",
      "like\n",
      "story\n",
      "time\n",
      "see\n",
      "bad\n",
      "first\n",
      "watch\n",
      "characters\n",
      "really\n",
      "gandhi\n",
      "re\n",
      "make\n",
      "think\n",
      "thought\n",
      "ever\n",
      "*************************\n",
      "topic: 8\n",
      "*************************\n",
      "film\n",
      "movie\n",
      "like\n",
      "one\n",
      "time\n",
      "even\n",
      "good\n",
      "much\n",
      "really\n",
      "show\n",
      "well\n",
      "see\n",
      "get\n",
      "think\n",
      "make\n",
      "great\n",
      "music\n",
      "also\n",
      "still\n",
      "bad\n",
      "*************************\n",
      "topic: 9\n",
      "*************************\n",
      "one\n",
      "macarthur\n",
      "film\n",
      "gen\n",
      "get\n",
      "war\n",
      "us\n",
      "story\n",
      "like\n",
      "home\n",
      "even\n",
      "point\n",
      "time\n",
      "military\n",
      "old\n",
      "though\n",
      "world\n",
      "give\n",
      "della\n",
      "back\n",
      "*************************\n",
      "topic: 10\n",
      "*************************\n",
      "howard\n",
      "one\n",
      "plot\n",
      "story\n",
      "scott\n",
      "windsor\n",
      "house\n",
      "beta\n",
      "scenes\n",
      "doctor\n",
      "bock\n",
      "friend\n",
      "mile\n",
      "john\n",
      "movie\n",
      "mills\n",
      "even\n",
      "naked\n",
      "like\n",
      "george\n",
      "*************************\n",
      "topic: 11\n",
      "*************************\n",
      "film\n",
      "people\n",
      "one\n",
      "movie\n",
      "like\n",
      "even\n",
      "world\n",
      "know\n",
      "austen\n",
      "emma\n",
      "story\n",
      "made\n",
      "stewart\n",
      "character\n",
      "first\n",
      "garbo\n",
      "much\n",
      "erika\n",
      "time\n",
      "great\n",
      "*************************\n",
      "topic: 12\n",
      "*************************\n",
      "film\n",
      "one\n",
      "movie\n",
      "like\n",
      "even\n",
      "good\n",
      "story\n",
      "well\n",
      "time\n",
      "first\n",
      "characters\n",
      "great\n",
      "much\n",
      "really\n",
      "see\n",
      "make\n",
      "bad\n",
      "love\n",
      "people\n",
      "get\n",
      "*************************\n",
      "topic: 13\n",
      "*************************\n",
      "film\n",
      "movie\n",
      "like\n",
      "one\n",
      "even\n",
      "good\n",
      "also\n",
      "time\n",
      "ripley\n",
      "bad\n",
      "love\n",
      "show\n",
      "two\n",
      "well\n",
      "made\n",
      "ever\n",
      "uncle\n",
      "never\n",
      "scene\n",
      "get\n",
      "*************************\n",
      "topic: 14\n",
      "*************************\n",
      "movie\n",
      "one\n",
      "film\n",
      "time\n",
      "man\n",
      "like\n",
      "story\n",
      "good\n",
      "get\n",
      "also\n",
      "life\n",
      "much\n",
      "make\n",
      "college\n",
      "chris\n",
      "first\n",
      "never\n",
      "new\n",
      "see\n",
      "comedy\n",
      "*************************\n",
      "topic: 15\n",
      "*************************\n",
      "movie\n",
      "like\n",
      "film\n",
      "one\n",
      "good\n",
      "story\n",
      "see\n",
      "even\n",
      "time\n",
      "make\n",
      "people\n",
      "films\n",
      "well\n",
      "really\n",
      "think\n",
      "much\n",
      "plot\n",
      "series\n",
      "never\n",
      "get\n",
      "*************************\n",
      "topic: 16\n",
      "*************************\n",
      "movie\n",
      "like\n",
      "film\n",
      "one\n",
      "even\n",
      "bad\n",
      "way\n",
      "totally\n",
      "really\n",
      "see\n",
      "think\n",
      "story\n",
      "back\n",
      "people\n",
      "around\n",
      "seen\n",
      "got\n",
      "watch\n",
      "well\n",
      "acting\n",
      "*************************\n",
      "topic: 17\n",
      "*************************\n",
      "movie\n",
      "film\n",
      "one\n",
      "like\n",
      "even\n",
      "characters\n",
      "really\n",
      "story\n",
      "get\n",
      "end\n",
      "made\n",
      "good\n",
      "make\n",
      "never\n",
      "love\n",
      "way\n",
      "funny\n",
      "re\n",
      "time\n",
      "watch\n",
      "*************************\n",
      "topic: 18\n",
      "*************************\n",
      "movie\n",
      "like\n",
      "film\n",
      "bourne\n",
      "good\n",
      "hospital\n",
      "miou\n",
      "young\n",
      "movies\n",
      "plot\n",
      "character\n",
      "also\n",
      "back\n",
      "see\n",
      "depardieu\n",
      "though\n",
      "makes\n",
      "loretta\n",
      "talented\n",
      "excellent\n",
      "*************************\n",
      "topic: 19\n",
      "*************************\n",
      "movie\n",
      "one\n",
      "story\n",
      "film\n",
      "time\n",
      "kells\n",
      "well\n",
      "many\n",
      "even\n",
      "scene\n",
      "girl\n",
      "really\n",
      "way\n",
      "makes\n",
      "show\n",
      "book\n",
      "man\n",
      "know\n",
      "falon\n",
      "done\n",
      "*************************\n",
      "topic: 20\n",
      "*************************\n",
      "film\n",
      "movie\n",
      "one\n",
      "story\n",
      "well\n",
      "like\n",
      "really\n",
      "even\n",
      "much\n",
      "time\n",
      "good\n",
      "way\n",
      "films\n",
      "m\n",
      "see\n",
      "seen\n",
      "first\n",
      "great\n",
      "people\n",
      "many\n",
      "*************************\n",
      "topic: 21\n",
      "*************************\n",
      "film\n",
      "one\n",
      "movie\n",
      "great\n",
      "story\n",
      "well\n",
      "many\n",
      "role\n",
      "see\n",
      "good\n",
      "really\n",
      "young\n",
      "gadget\n",
      "end\n",
      "cartoon\n",
      "seen\n",
      "scene\n",
      "ever\n",
      "man\n",
      "matthau\n",
      "*************************\n",
      "topic: 22\n",
      "*************************\n",
      "movie\n",
      "good\n",
      "like\n",
      "one\n",
      "film\n",
      "bad\n",
      "even\n",
      "time\n",
      "really\n",
      "story\n",
      "get\n",
      "people\n",
      "make\n",
      "movies\n",
      "see\n",
      "seen\n",
      "little\n",
      "acting\n",
      "also\n",
      "much\n",
      "*************************\n",
      "topic: 23\n",
      "*************************\n",
      "film\n",
      "movie\n",
      "one\n",
      "like\n",
      "see\n",
      "really\n",
      "time\n",
      "story\n",
      "seen\n",
      "good\n",
      "people\n",
      "many\n",
      "think\n",
      "ve\n",
      "even\n",
      "get\n",
      "actors\n",
      "great\n",
      "man\n",
      "ever\n",
      "*************************\n",
      "topic: 24\n",
      "*************************\n",
      "film\n",
      "movie\n",
      "one\n",
      "like\n",
      "good\n",
      "even\n",
      "see\n",
      "people\n",
      "also\n",
      "time\n",
      "story\n",
      "love\n",
      "way\n",
      "make\n",
      "well\n",
      "think\n",
      "much\n",
      "get\n",
      "man\n",
      "scene\n",
      "*************************\n",
      "topic: 25\n",
      "*************************\n",
      "one\n",
      "magoo\n",
      "even\n",
      "tarzan\n",
      "mr\n",
      "film\n",
      "movie\n",
      "like\n",
      "man\n",
      "well\n",
      "jane\n",
      "make\n",
      "films\n",
      "good\n",
      "time\n",
      "series\n",
      "much\n",
      "funny\n",
      "young\n",
      "bad\n",
      "*************************\n",
      "topic: 26\n",
      "*************************\n",
      "film\n",
      "one\n",
      "movie\n",
      "horror\n",
      "like\n",
      "first\n",
      "story\n",
      "house\n",
      "time\n",
      "back\n",
      "re\n",
      "good\n",
      "little\n",
      "last\n",
      "best\n",
      "makes\n",
      "war\n",
      "also\n",
      "ever\n",
      "zombie\n",
      "*************************\n",
      "topic: 27\n",
      "*************************\n",
      "film\n",
      "good\n",
      "like\n",
      "movie\n",
      "also\n",
      "one\n",
      "plot\n",
      "character\n",
      "characters\n",
      "even\n",
      "two\n",
      "nimi\n",
      "jimmy\n",
      "matt\n",
      "films\n",
      "colin\n",
      "action\n",
      "nothing\n",
      "scene\n",
      "sammy\n",
      "*************************\n",
      "topic: 28\n",
      "*************************\n",
      "movie\n",
      "one\n",
      "film\n",
      "time\n",
      "like\n",
      "see\n",
      "bad\n",
      "get\n",
      "well\n",
      "really\n",
      "much\n",
      "make\n",
      "good\n",
      "even\n",
      "also\n",
      "story\n",
      "many\n",
      "people\n",
      "best\n",
      "way\n",
      "*************************\n",
      "topic: 29\n",
      "*************************\n",
      "movie\n",
      "film\n",
      "one\n",
      "like\n",
      "good\n",
      "time\n",
      "even\n",
      "see\n",
      "really\n",
      "story\n",
      "well\n",
      "much\n",
      "get\n",
      "bad\n",
      "great\n",
      "also\n",
      "people\n",
      "first\n",
      "made\n",
      "make\n",
      "*************************\n"
     ]
    }
   ],
   "source": [
    "#Code to extract topics from models\n",
    "vectorized_model = pipeline_model.stages[2]\n",
    "topic_model = pipeline_model.stages[3]\n",
    "vocab = vectorized_model.vocabulary\n",
    "topic_words_list = topic_model.describeTopics(20)\n",
    "topic_words_rdd = topic_words_list.rdd\n",
    "topics_words = topic_words_rdd.map(lambda row: row['termIndices']).map(lambda idx_list: [vocab[idx] for idx in idx_list]).collect()\n",
    "\n",
    "for idx, topic in enumerate(topics_words):\n",
    "    print(\"topic: {}\".format(idx))\n",
    "    print(\"*\"*25)\n",
    "    for word in topic:\n",
    "       print(word)\n",
    "    print(\"*\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whole-genealogy",
   "metadata": {},
   "source": [
    "How do you think about the topics? Do they make sense? If you think the topics we get from the movie reviews should be better, let's continue to see what we can do to make them better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-paste",
   "metadata": {},
   "source": [
    "One possible reason is that we have many words that do not show up frequently. That is, they are very specific words to certain movies but don't occur across reviews. Such words are not very meaningful and they do not represent common themes in those reviews. So here we limit the frequency of words to at least 5 and run LDA with pipeline again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "informative-share",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter the countvectorizer\n",
    "tf = ft.CountVectorizer(inputCol=stopwords.getOutputCol(), outputCol='review_tf', minDF=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "amino-pathology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(topicDistribution=DenseVector([0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.9946])),\n",
       " Row(topicDistribution=DenseVector([0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.9894])),\n",
       " Row(topicDistribution=DenseVector([0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.9895])),\n",
       " Row(topicDistribution=DenseVector([0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.9865])),\n",
       " Row(topicDistribution=DenseVector([0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.9927]))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[Your Code] to build a ML pipeline and fit LDA again\n",
    "\n",
    "\n",
    "# Create the pipeline\n",
    "lda_pipeline = Pipeline(stages=pipeline_stages)\n",
    "\n",
    "# Fit the pipeline to the data\n",
    "pipeline_model = lda_pipeline.fit(reviews)\n",
    "\n",
    "topics = pipeline_model.transform(reviews)\n",
    "topics.select('topicDistribution').take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "union-corps",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic: 0\n",
      "*************************\n",
      "film\n",
      "movie\n",
      "one\n",
      "time\n",
      "made\n",
      "good\n",
      "like\n",
      "bad\n",
      "films\n",
      "get\n",
      "two\n",
      "always\n",
      "first\n",
      "plot\n",
      "father\n",
      "little\n",
      "years\n",
      "also\n",
      "cast\n",
      "life\n",
      "*************************\n",
      "topic: 1\n",
      "*************************\n",
      "movie\n",
      "bad\n",
      "part\n",
      "film\n",
      "one\n",
      "like\n",
      "us\n",
      "still\n",
      "time\n",
      "character\n",
      "story\n",
      "good\n",
      "much\n",
      "two\n",
      "even\n",
      "horizon\n",
      "end\n",
      "make\n",
      "lost\n",
      "fuller\n",
      "*************************\n",
      "topic: 2\n",
      "*************************\n",
      "film\n",
      "one\n",
      "movie\n",
      "good\n",
      "like\n",
      "well\n",
      "great\n",
      "never\n",
      "show\n",
      "first\n",
      "really\n",
      "story\n",
      "also\n",
      "see\n",
      "made\n",
      "much\n",
      "get\n",
      "watch\n",
      "time\n",
      "two\n",
      "*************************\n",
      "topic: 3\n",
      "*************************\n",
      "movie\n",
      "one\n",
      "film\n",
      "series\n",
      "first\n",
      "story\n",
      "movies\n",
      "good\n",
      "really\n",
      "like\n",
      "much\n",
      "watch\n",
      "little\n",
      "puppet\n",
      "get\n",
      "also\n",
      "even\n",
      "time\n",
      "book\n",
      "made\n",
      "*************************\n",
      "topic: 4\n",
      "*************************\n",
      "movie\n",
      "one\n",
      "like\n",
      "film\n",
      "movies\n",
      "great\n",
      "bad\n",
      "also\n",
      "really\n",
      "story\n",
      "good\n",
      "get\n",
      "made\n",
      "much\n",
      "even\n",
      "scene\n",
      "well\n",
      "see\n",
      "think\n",
      "characters\n",
      "*************************\n",
      "topic: 5\n",
      "*************************\n",
      "movie\n",
      "film\n",
      "one\n",
      "like\n",
      "really\n",
      "good\n",
      "even\n",
      "many\n",
      "movies\n",
      "see\n",
      "time\n",
      "well\n",
      "great\n",
      "films\n",
      "make\n",
      "story\n",
      "made\n",
      "first\n",
      "character\n",
      "plot\n",
      "*************************\n",
      "topic: 6\n",
      "*************************\n",
      "movie\n",
      "film\n",
      "good\n",
      "one\n",
      "like\n",
      "time\n",
      "even\n",
      "first\n",
      "also\n",
      "well\n",
      "made\n",
      "scene\n",
      "get\n",
      "much\n",
      "never\n",
      "people\n",
      "old\n",
      "plot\n",
      "story\n",
      "jane\n",
      "*************************\n",
      "topic: 7\n",
      "*************************\n",
      "movie\n",
      "one\n",
      "good\n",
      "film\n",
      "show\n",
      "like\n",
      "story\n",
      "time\n",
      "see\n",
      "bad\n",
      "first\n",
      "watch\n",
      "characters\n",
      "really\n",
      "gandhi\n",
      "re\n",
      "make\n",
      "think\n",
      "thought\n",
      "ever\n",
      "*************************\n",
      "topic: 8\n",
      "*************************\n",
      "film\n",
      "movie\n",
      "like\n",
      "one\n",
      "time\n",
      "even\n",
      "good\n",
      "much\n",
      "really\n",
      "show\n",
      "well\n",
      "see\n",
      "get\n",
      "think\n",
      "make\n",
      "great\n",
      "music\n",
      "also\n",
      "still\n",
      "bad\n",
      "*************************\n",
      "topic: 9\n",
      "*************************\n",
      "one\n",
      "macarthur\n",
      "film\n",
      "gen\n",
      "get\n",
      "war\n",
      "us\n",
      "story\n",
      "like\n",
      "home\n",
      "even\n",
      "point\n",
      "time\n",
      "military\n",
      "old\n",
      "though\n",
      "world\n",
      "give\n",
      "della\n",
      "back\n",
      "*************************\n",
      "topic: 10\n",
      "*************************\n",
      "howard\n",
      "one\n",
      "plot\n",
      "story\n",
      "scott\n",
      "windsor\n",
      "house\n",
      "beta\n",
      "scenes\n",
      "doctor\n",
      "bock\n",
      "friend\n",
      "mile\n",
      "john\n",
      "movie\n",
      "mills\n",
      "even\n",
      "naked\n",
      "like\n",
      "george\n",
      "*************************\n",
      "topic: 11\n",
      "*************************\n",
      "film\n",
      "people\n",
      "one\n",
      "movie\n",
      "like\n",
      "even\n",
      "world\n",
      "know\n",
      "austen\n",
      "emma\n",
      "story\n",
      "made\n",
      "stewart\n",
      "character\n",
      "first\n",
      "garbo\n",
      "much\n",
      "erika\n",
      "time\n",
      "great\n",
      "*************************\n",
      "topic: 12\n",
      "*************************\n",
      "film\n",
      "one\n",
      "movie\n",
      "like\n",
      "even\n",
      "good\n",
      "story\n",
      "well\n",
      "time\n",
      "first\n",
      "characters\n",
      "great\n",
      "much\n",
      "really\n",
      "see\n",
      "make\n",
      "bad\n",
      "love\n",
      "people\n",
      "get\n",
      "*************************\n",
      "topic: 13\n",
      "*************************\n",
      "film\n",
      "movie\n",
      "like\n",
      "one\n",
      "even\n",
      "good\n",
      "also\n",
      "time\n",
      "ripley\n",
      "bad\n",
      "love\n",
      "show\n",
      "two\n",
      "well\n",
      "made\n",
      "ever\n",
      "uncle\n",
      "never\n",
      "scene\n",
      "get\n",
      "*************************\n",
      "topic: 14\n",
      "*************************\n",
      "movie\n",
      "one\n",
      "film\n",
      "time\n",
      "man\n",
      "like\n",
      "story\n",
      "good\n",
      "get\n",
      "also\n",
      "life\n",
      "much\n",
      "make\n",
      "college\n",
      "chris\n",
      "first\n",
      "never\n",
      "new\n",
      "see\n",
      "comedy\n",
      "*************************\n",
      "topic: 15\n",
      "*************************\n",
      "movie\n",
      "like\n",
      "film\n",
      "one\n",
      "good\n",
      "story\n",
      "see\n",
      "even\n",
      "time\n",
      "make\n",
      "people\n",
      "films\n",
      "well\n",
      "really\n",
      "think\n",
      "much\n",
      "plot\n",
      "series\n",
      "never\n",
      "get\n",
      "*************************\n",
      "topic: 16\n",
      "*************************\n",
      "movie\n",
      "like\n",
      "film\n",
      "one\n",
      "even\n",
      "bad\n",
      "way\n",
      "totally\n",
      "really\n",
      "see\n",
      "think\n",
      "story\n",
      "back\n",
      "people\n",
      "around\n",
      "seen\n",
      "got\n",
      "watch\n",
      "well\n",
      "acting\n",
      "*************************\n",
      "topic: 17\n",
      "*************************\n",
      "movie\n",
      "film\n",
      "one\n",
      "like\n",
      "even\n",
      "characters\n",
      "really\n",
      "story\n",
      "get\n",
      "end\n",
      "made\n",
      "good\n",
      "make\n",
      "never\n",
      "love\n",
      "way\n",
      "funny\n",
      "re\n",
      "time\n",
      "watch\n",
      "*************************\n",
      "topic: 18\n",
      "*************************\n",
      "movie\n",
      "like\n",
      "film\n",
      "bourne\n",
      "good\n",
      "hospital\n",
      "miou\n",
      "young\n",
      "movies\n",
      "plot\n",
      "character\n",
      "also\n",
      "back\n",
      "see\n",
      "depardieu\n",
      "though\n",
      "makes\n",
      "loretta\n",
      "talented\n",
      "excellent\n",
      "*************************\n",
      "topic: 19\n",
      "*************************\n",
      "movie\n",
      "one\n",
      "story\n",
      "film\n",
      "time\n",
      "kells\n",
      "well\n",
      "many\n",
      "even\n",
      "scene\n",
      "girl\n",
      "really\n",
      "way\n",
      "makes\n",
      "show\n",
      "book\n",
      "man\n",
      "know\n",
      "falon\n",
      "done\n",
      "*************************\n",
      "topic: 20\n",
      "*************************\n",
      "film\n",
      "movie\n",
      "one\n",
      "story\n",
      "well\n",
      "like\n",
      "really\n",
      "even\n",
      "much\n",
      "time\n",
      "good\n",
      "way\n",
      "films\n",
      "m\n",
      "see\n",
      "seen\n",
      "first\n",
      "great\n",
      "people\n",
      "many\n",
      "*************************\n",
      "topic: 21\n",
      "*************************\n",
      "film\n",
      "one\n",
      "movie\n",
      "great\n",
      "story\n",
      "well\n",
      "many\n",
      "role\n",
      "see\n",
      "good\n",
      "really\n",
      "young\n",
      "gadget\n",
      "end\n",
      "cartoon\n",
      "seen\n",
      "scene\n",
      "ever\n",
      "man\n",
      "matthau\n",
      "*************************\n",
      "topic: 22\n",
      "*************************\n",
      "movie\n",
      "good\n",
      "like\n",
      "one\n",
      "film\n",
      "bad\n",
      "even\n",
      "time\n",
      "really\n",
      "story\n",
      "get\n",
      "people\n",
      "make\n",
      "movies\n",
      "see\n",
      "seen\n",
      "little\n",
      "acting\n",
      "also\n",
      "much\n",
      "*************************\n",
      "topic: 23\n",
      "*************************\n",
      "film\n",
      "movie\n",
      "one\n",
      "like\n",
      "see\n",
      "really\n",
      "time\n",
      "story\n",
      "seen\n",
      "good\n",
      "people\n",
      "many\n",
      "think\n",
      "ve\n",
      "even\n",
      "get\n",
      "actors\n",
      "great\n",
      "man\n",
      "ever\n",
      "*************************\n",
      "topic: 24\n",
      "*************************\n",
      "film\n",
      "movie\n",
      "one\n",
      "like\n",
      "good\n",
      "even\n",
      "see\n",
      "people\n",
      "also\n",
      "time\n",
      "story\n",
      "love\n",
      "way\n",
      "make\n",
      "well\n",
      "think\n",
      "much\n",
      "get\n",
      "man\n",
      "scene\n",
      "*************************\n",
      "topic: 25\n",
      "*************************\n",
      "one\n",
      "magoo\n",
      "even\n",
      "tarzan\n",
      "mr\n",
      "film\n",
      "movie\n",
      "like\n",
      "man\n",
      "well\n",
      "jane\n",
      "make\n",
      "films\n",
      "good\n",
      "time\n",
      "series\n",
      "much\n",
      "funny\n",
      "young\n",
      "bad\n",
      "*************************\n",
      "topic: 26\n",
      "*************************\n",
      "film\n",
      "one\n",
      "movie\n",
      "horror\n",
      "like\n",
      "first\n",
      "story\n",
      "house\n",
      "time\n",
      "back\n",
      "re\n",
      "good\n",
      "little\n",
      "last\n",
      "best\n",
      "makes\n",
      "war\n",
      "also\n",
      "ever\n",
      "zombie\n",
      "*************************\n",
      "topic: 27\n",
      "*************************\n",
      "film\n",
      "good\n",
      "like\n",
      "movie\n",
      "also\n",
      "one\n",
      "plot\n",
      "character\n",
      "characters\n",
      "even\n",
      "two\n",
      "nimi\n",
      "jimmy\n",
      "matt\n",
      "films\n",
      "colin\n",
      "action\n",
      "nothing\n",
      "scene\n",
      "sammy\n",
      "*************************\n",
      "topic: 28\n",
      "*************************\n",
      "movie\n",
      "one\n",
      "film\n",
      "time\n",
      "like\n",
      "see\n",
      "bad\n",
      "get\n",
      "well\n",
      "really\n",
      "much\n",
      "make\n",
      "good\n",
      "even\n",
      "also\n",
      "story\n",
      "many\n",
      "people\n",
      "best\n",
      "way\n",
      "*************************\n",
      "topic: 29\n",
      "*************************\n",
      "movie\n",
      "film\n",
      "one\n",
      "like\n",
      "good\n",
      "time\n",
      "even\n",
      "see\n",
      "really\n",
      "story\n",
      "well\n",
      "much\n",
      "get\n",
      "bad\n",
      "great\n",
      "also\n",
      "people\n",
      "first\n",
      "made\n",
      "make\n",
      "*************************\n"
     ]
    }
   ],
   "source": [
    "#Code to extract topics from models\n",
    "vectorized_model = pipeline_model.stages[2]\n",
    "topic_model = pipeline_model.stages[3]\n",
    "vocab = vectorized_model.vocabulary\n",
    "topic_words_list = topic_model.describeTopics(20)\n",
    "topic_words_rdd = topic_words_list.rdd\n",
    "topics_words = topic_words_rdd.map(lambda row: row['termIndices']).map(lambda idx_list: [vocab[idx] for idx in idx_list]).collect()\n",
    "\n",
    "for idx, topic in enumerate(topics_words):\n",
    "    print(\"topic: {}\".format(idx))\n",
    "    print(\"*\"*25)\n",
    "    for word in topic:\n",
    "       print(word)\n",
    "    print(\"*\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-semiconductor",
   "metadata": {},
   "source": [
    "It is expected that the topics are getting better but still not very satisfying. Some words may be very specific to some reviews. Also, there are lots of words shown in different topics many times; possibly they are too common so they shouldn't be that important. Let's take one more step to use TF-IDF vector rather than TF vector. To build IF-IDF, we first create TF with CountVectorizer then create IDF from TF vector. Then we run LDA model with IF-IDF vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "latest-making",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use tf-idf vector\n",
    "tf = ft.CountVectorizer(inputCol=stopwords.getOutputCol(), outputCol=\"review_tf\", vocabSize=10000)\n",
    "idf = ft.IDF(inputCol=tf.getOutputCol(), outputCol=\"review_tfidf\", minDocFreq=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "strange-purchase",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(topicDistribution=DenseVector([0.0002, 0.0002, 0.2916, 0.0002, 0.5871, 0.0002, 0.0002, 0.1156, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002])),\n",
       " Row(topicDistribution=DenseVector([0.0004, 0.0004, 0.1968, 0.0004, 0.7915, 0.0004, 0.0004, 0.0005, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004])),\n",
       " Row(topicDistribution=DenseVector([0.0004, 0.0004, 0.1917, 0.0004, 0.7977, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004, 0.0004])),\n",
       " Row(topicDistribution=DenseVector([0.0005, 0.0005, 0.0005, 0.0005, 0.8901, 0.0005, 0.0005, 0.0964, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005, 0.0005])),\n",
       " Row(topicDistribution=DenseVector([0.0003, 0.0003, 0.0003, 0.0003, 0.7433, 0.0003, 0.0003, 0.2484, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003]))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#[Your Code] to create a LDA model (30 topics) and put everything together into a ML pipeline to fit LDA\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Define stages of the pipeline\n",
    "pipeline_stages = [\n",
    "    # Tokenize the review text\n",
    "    tokenizer,\n",
    "    # Remove stopwords\n",
    "    stopwords,\n",
    "    # Convert text into a sparse vector of token counts\n",
    "    tf,\n",
    "    idf,\n",
    "    # Fit LDA model\n",
    "    lda\n",
    "]\n",
    "\n",
    "# Create the pipeline\n",
    "lda_pipeline = Pipeline(stages=pipeline_stages)\n",
    "\n",
    "# Fit the pipeline to the data\n",
    "pipeline_model = lda_pipeline.fit(reviews)\n",
    "\n",
    "topics = pipeline_model.transform(reviews)\n",
    "topics.select('topicDistribution').take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "headed-placement",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic: 0\n",
      "*************************\n",
      "film\n",
      "carmen\n",
      "one\n",
      "movie\n",
      "freeman\n",
      "made\n",
      "vega\n",
      "kar\n",
      "wai\n",
      "time\n",
      "films\n",
      "good\n",
      "scarlet\n",
      "paz\n",
      "like\n",
      "first\n",
      "love\n",
      "lau\n",
      "bad\n",
      "however\n",
      "*************************\n",
      "topic: 1\n",
      "*************************\n",
      "movie\n",
      "streisand\n",
      "one\n",
      "barbra\n",
      "film\n",
      "like\n",
      "bugs\n",
      "prom\n",
      "donna\n",
      "bad\n",
      "even\n",
      "best\n",
      "good\n",
      "also\n",
      "scenes\n",
      "movies\n",
      "see\n",
      "action\n",
      "much\n",
      "know\n",
      "*************************\n",
      "topic: 2\n",
      "*************************\n",
      "film\n",
      "one\n",
      "like\n",
      "movie\n",
      "even\n",
      "time\n",
      "man\n",
      "good\n",
      "also\n",
      "get\n",
      "story\n",
      "well\n",
      "back\n",
      "two\n",
      "great\n",
      "see\n",
      "films\n",
      "make\n",
      "new\n",
      "much\n",
      "*************************\n",
      "topic: 3\n",
      "*************************\n",
      "film\n",
      "dreyfuss\n",
      "dictator\n",
      "movie\n",
      "nevsky\n",
      "time\n",
      "one\n",
      "eisenstein\n",
      "jonathan\n",
      "julia\n",
      "get\n",
      "see\n",
      "really\n",
      "also\n",
      "know\n",
      "characters\n",
      "good\n",
      "much\n",
      "story\n",
      "man\n",
      "*************************\n",
      "topic: 4\n",
      "*************************\n",
      "film\n",
      "movie\n",
      "one\n",
      "like\n",
      "good\n",
      "time\n",
      "even\n",
      "really\n",
      "see\n",
      "story\n",
      "much\n",
      "well\n",
      "bad\n",
      "get\n",
      "people\n",
      "great\n",
      "first\n",
      "also\n",
      "movies\n",
      "characters\n",
      "*************************\n",
      "topic: 5\n",
      "*************************\n",
      "bam\n",
      "jackass\n",
      "one\n",
      "movie\n",
      "give\n",
      "viva\n",
      "make\n",
      "two\n",
      "much\n",
      "war\n",
      "town\n",
      "good\n",
      "film\n",
      "even\n",
      "thing\n",
      "people\n",
      "every\n",
      "better\n",
      "know\n",
      "ll\n",
      "*************************\n",
      "topic: 6\n",
      "*************************\n",
      "cagney\n",
      "lil\n",
      "marlene\n",
      "jason\n",
      "summer\n",
      "demon\n",
      "shanghai\n",
      "whose\n",
      "berkeley\n",
      "numbers\n",
      "ruby\n",
      "blondell\n",
      "film\n",
      "kingsley\n",
      "ricky\n",
      "chicken\n",
      "musicals\n",
      "ancient\n",
      "movie\n",
      "tree\n",
      "*************************\n",
      "topic: 7\n",
      "*************************\n",
      "movie\n",
      "one\n",
      "film\n",
      "like\n",
      "life\n",
      "time\n",
      "story\n",
      "love\n",
      "see\n",
      "even\n",
      "good\n",
      "really\n",
      "people\n",
      "family\n",
      "well\n",
      "great\n",
      "never\n",
      "made\n",
      "way\n",
      "first\n",
      "*************************\n",
      "topic: 8\n",
      "*************************\n",
      "film\n",
      "one\n",
      "even\n",
      "like\n",
      "movie\n",
      "time\n",
      "show\n",
      "war\n",
      "though\n",
      "people\n",
      "get\n",
      "story\n",
      "scenes\n",
      "great\n",
      "two\n",
      "also\n",
      "long\n",
      "well\n",
      "good\n",
      "much\n",
      "*************************\n",
      "topic: 9\n",
      "*************************\n",
      "film\n",
      "alexandre\n",
      "one\n",
      "time\n",
      "like\n",
      "darwin\n",
      "much\n",
      "veronika\n",
      "well\n",
      "movie\n",
      "even\n",
      "however\n",
      "story\n",
      "people\n",
      "woman\n",
      "vincenzo\n",
      "characters\n",
      "long\n",
      "say\n",
      "many\n",
      "*************************\n",
      "topic: 10\n",
      "*************************\n",
      "film\n",
      "one\n",
      "game\n",
      "war\n",
      "like\n",
      "kahn\n",
      "well\n",
      "human\n",
      "nathaniel\n",
      "life\n",
      "lincoln\n",
      "story\n",
      "even\n",
      "good\n",
      "jacobi\n",
      "show\n",
      "play\n",
      "movie\n",
      "father\n",
      "great\n",
      "*************************\n",
      "topic: 11\n",
      "*************************\n",
      "yeti\n",
      "film\n",
      "puerto\n",
      "crisis\n",
      "movie\n",
      "one\n",
      "spiritual\n",
      "island\n",
      "painful\n",
      "allowed\n",
      "nation\n",
      "man\n",
      "yet\n",
      "mike\n",
      "friend\n",
      "best\n",
      "made\n",
      "fortunately\n",
      "rampant\n",
      "says\n",
      "*************************\n",
      "topic: 12\n",
      "*************************\n",
      "movie\n",
      "film\n",
      "one\n",
      "even\n",
      "jane\n",
      "bad\n",
      "really\n",
      "well\n",
      "like\n",
      "good\n",
      "time\n",
      "see\n",
      "story\n",
      "plot\n",
      "something\n",
      "two\n",
      "scene\n",
      "first\n",
      "made\n",
      "also\n",
      "*************************\n",
      "topic: 13\n",
      "*************************\n",
      "movie\n",
      "like\n",
      "one\n",
      "good\n",
      "white\n",
      "series\n",
      "black\n",
      "see\n",
      "get\n",
      "music\n",
      "film\n",
      "garfield\n",
      "new\n",
      "much\n",
      "never\n",
      "great\n",
      "cat\n",
      "made\n",
      "man\n",
      "kids\n",
      "*************************\n",
      "topic: 14\n",
      "*************************\n",
      "film\n",
      "movie\n",
      "one\n",
      "snipes\n",
      "bettie\n",
      "really\n",
      "wesley\n",
      "story\n",
      "good\n",
      "like\n",
      "character\n",
      "see\n",
      "much\n",
      "many\n",
      "films\n",
      "people\n",
      "love\n",
      "characters\n",
      "well\n",
      "original\n",
      "*************************\n",
      "topic: 15\n",
      "*************************\n",
      "movie\n",
      "one\n",
      "cinderella\n",
      "like\n",
      "film\n",
      "kelly\n",
      "even\n",
      "really\n",
      "story\n",
      "movies\n",
      "paris\n",
      "good\n",
      "way\n",
      "time\n",
      "see\n",
      "re\n",
      "gershwin\n",
      "make\n",
      "ever\n",
      "caron\n",
      "*************************\n",
      "topic: 16\n",
      "*************************\n",
      "christy\n",
      "one\n",
      "barney\n",
      "film\n",
      "heart\n",
      "hayworth\n",
      "like\n",
      "movie\n",
      "kelly\n",
      "love\n",
      "way\n",
      "real\n",
      "even\n",
      "care\n",
      "good\n",
      "rita\n",
      "story\n",
      "people\n",
      "get\n",
      "also\n",
      "*************************\n",
      "topic: 17\n",
      "*************************\n",
      "mencia\n",
      "helen\n",
      "andrews\n",
      "show\n",
      "really\n",
      "tierney\n",
      "film\n",
      "time\n",
      "one\n",
      "like\n",
      "much\n",
      "people\n",
      "carlos\n",
      "movie\n",
      "bye\n",
      "sidewalk\n",
      "dana\n",
      "films\n",
      "dixon\n",
      "way\n",
      "*************************\n",
      "topic: 18\n",
      "*************************\n",
      "vs\n",
      "gina\n",
      "5\n",
      "match\n",
      "w\n",
      "one\n",
      "crowd\n",
      "really\n",
      "game\n",
      "wins\n",
      "gets\n",
      "get\n",
      "tim\n",
      "2\n",
      "cops\n",
      "movie\n",
      "good\n",
      "1\n",
      "nothing\n",
      "time\n",
      "*************************\n",
      "topic: 19\n",
      "*************************\n",
      "fido\n",
      "film\n",
      "one\n",
      "movie\n",
      "like\n",
      "timmy\n",
      "dixon\n",
      "zombie\n",
      "love\n",
      "really\n",
      "even\n",
      "good\n",
      "bad\n",
      "jimmy\n",
      "show\n",
      "see\n",
      "story\n",
      "well\n",
      "great\n",
      "way\n",
      "*************************\n",
      "topic: 20\n",
      "*************************\n",
      "karloff\n",
      "rukh\n",
      "one\n",
      "lugosi\n",
      "film\n",
      "good\n",
      "well\n",
      "like\n",
      "two\n",
      "puppet\n",
      "first\n",
      "movie\n",
      "even\n",
      "also\n",
      "see\n",
      "story\n",
      "characters\n",
      "effects\n",
      "much\n",
      "people\n",
      "*************************\n",
      "topic: 21\n",
      "*************************\n",
      "one\n",
      "match\n",
      "film\n",
      "like\n",
      "well\n",
      "show\n",
      "love\n",
      "john\n",
      "movie\n",
      "man\n",
      "comedy\n",
      "best\n",
      "really\n",
      "scene\n",
      "vs\n",
      "time\n",
      "also\n",
      "story\n",
      "life\n",
      "even\n",
      "*************************\n",
      "topic: 22\n",
      "*************************\n",
      "film\n",
      "one\n",
      "movie\n",
      "like\n",
      "also\n",
      "really\n",
      "films\n",
      "house\n",
      "horror\n",
      "good\n",
      "shark\n",
      "story\n",
      "sammo\n",
      "people\n",
      "great\n",
      "well\n",
      "characters\n",
      "real\n",
      "bad\n",
      "many\n",
      "*************************\n",
      "topic: 23\n",
      "*************************\n",
      "movie\n",
      "one\n",
      "like\n",
      "che\n",
      "see\n",
      "film\n",
      "great\n",
      "scooby\n",
      "time\n",
      "people\n",
      "also\n",
      "doo\n",
      "really\n",
      "show\n",
      "way\n",
      "ever\n",
      "seen\n",
      "characters\n",
      "first\n",
      "character\n",
      "*************************\n",
      "topic: 24\n",
      "*************************\n",
      "film\n",
      "movie\n",
      "like\n",
      "one\n",
      "cassavetes\n",
      "good\n",
      "even\n",
      "well\n",
      "see\n",
      "much\n",
      "mabel\n",
      "films\n",
      "gundam\n",
      "ants\n",
      "characters\n",
      "make\n",
      "people\n",
      "watching\n",
      "never\n",
      "powell\n",
      "*************************\n",
      "topic: 25\n",
      "*************************\n",
      "movie\n",
      "film\n",
      "one\n",
      "bond\n",
      "custer\n",
      "like\n",
      "time\n",
      "good\n",
      "also\n",
      "see\n",
      "even\n",
      "make\n",
      "never\n",
      "really\n",
      "get\n",
      "made\n",
      "much\n",
      "story\n",
      "best\n",
      "crystal\n",
      "*************************\n",
      "topic: 26\n",
      "*************************\n",
      "movie\n",
      "chan\n",
      "film\n",
      "one\n",
      "jackie\n",
      "emma\n",
      "like\n",
      "modesty\n",
      "even\n",
      "story\n",
      "scorpion\n",
      "police\n",
      "made\n",
      "huppert\n",
      "much\n",
      "character\n",
      "movies\n",
      "get\n",
      "make\n",
      "really\n",
      "*************************\n",
      "topic: 27\n",
      "*************************\n",
      "movie\n",
      "one\n",
      "snakes\n",
      "like\n",
      "bad\n",
      "novel\n",
      "get\n",
      "jane\n",
      "scenes\n",
      "well\n",
      "film\n",
      "see\n",
      "rochester\n",
      "best\n",
      "go\n",
      "scene\n",
      "performance\n",
      "even\n",
      "granger\n",
      "us\n",
      "*************************\n",
      "topic: 28\n",
      "*************************\n",
      "movie\n",
      "film\n",
      "one\n",
      "like\n",
      "good\n",
      "story\n",
      "much\n",
      "time\n",
      "even\n",
      "made\n",
      "also\n",
      "get\n",
      "great\n",
      "see\n",
      "plot\n",
      "thing\n",
      "marlow\n",
      "well\n",
      "movies\n",
      "roy\n",
      "*************************\n",
      "topic: 29\n",
      "*************************\n",
      "one\n",
      "columbo\n",
      "movie\n",
      "stooges\n",
      "film\n",
      "good\n",
      "show\n",
      "like\n",
      "time\n",
      "curly\n",
      "series\n",
      "made\n",
      "really\n",
      "great\n",
      "people\n",
      "well\n",
      "shorts\n",
      "shemp\n",
      "even\n",
      "watch\n",
      "*************************\n"
     ]
    }
   ],
   "source": [
    "#Code to extract topics from models\n",
    "tf_model = pipeline_model.stages[2]\n",
    "topic_model = pipeline_model.stages[4]\n",
    "vocab = tf_model.vocabulary\n",
    "topic_words_list = topic_model.describeTopics(20)\n",
    "topic_words_rdd = topic_words_list.rdd\n",
    "topics_words = topic_words_rdd.map(lambda row: row['termIndices']).map(lambda idx_list: [vocab[idx] for idx in idx_list]).collect()\n",
    "\n",
    "for idx, topic in enumerate(topics_words):\n",
    "    print(\"topic: {}\".format(idx))\n",
    "    print(\"*\"*25)\n",
    "    for word in topic:\n",
    "       print(word)\n",
    "    print(\"*\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-notification",
   "metadata": {},
   "source": [
    "The topics should be more reasonable now. You should believe they can still be further improved by cleaning up the text and tuning the hyperparameters but let's stop here for assignment. If you want to try yourself beyond the assignment, you can change the model configuration to see if you can get any further improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "otherwise-regular",
   "metadata": {},
   "source": [
    "### Task 2 - Movie Review Sentiment Analysis with Spark ML\n",
    "The second task we are going to prediction. Let's continue with the reviews data and now we can do sentiment analysis with the TF-IDF. So with the TF-IDF vector, we can train and predict review sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "infrared-anime",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|sentiment|\n",
      "+---------+\n",
      "| positive|\n",
      "| negative|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#first let's confirm the potential labels\n",
    "#(it is possible sentiment can be neutral so we should make sure if that's the case)\n",
    "reviews.select('sentiment').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "asian-halifax",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+---------------+\n",
      "|              review|sentiment|sentiment_label|\n",
      "+--------------------+---------+---------------+\n",
      "|One of the other ...| positive|            1.0|\n",
      "|A wonderful littl...| positive|            1.0|\n",
      "|I thought this wa...| positive|            1.0|\n",
      "|Basically there s...| negative|            0.0|\n",
      "|Petter Mattei s  ...| positive|            1.0|\n",
      "|Probably my all t...| positive|            1.0|\n",
      "|I sure would like...| positive|            1.0|\n",
      "|This show was an ...| negative|            0.0|\n",
      "|Encouraged by the...| negative|            0.0|\n",
      "|If you like origi...| positive|            1.0|\n",
      "|Phil the Alien is...| negative|            0.0|\n",
      "|I saw this movie ...| negative|            0.0|\n",
      "|So im not a big f...| negative|            0.0|\n",
      "|The cast played S...| negative|            0.0|\n",
      "|This a fantastic ...| positive|            1.0|\n",
      "|Kind of drawn in ...| negative|            0.0|\n",
      "|Some films just s...| positive|            1.0|\n",
      "|This movie made i...| negative|            0.0|\n",
      "|I remember this f...| positive|            1.0|\n",
      "|An awful film  It...| negative|            0.0|\n",
      "+--------------------+---------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#let's create the binary numerical lable from postive/negative\n",
    "reviews = reviews.withColumn('sentiment_label', fn.when(fn.col('sentiment')=='positive', 1.0).otherwise(0.0))\n",
    "reviews.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "social-dream",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the training and testing set, with 80/20\n",
    "reviews_train, reviews_test = reviews.randomSplit([0.8, 0.2], seed=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "present-tiger",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.classification as cl\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "#create logistic gression model and then build the pipeline to train the model\n",
    "lr = cl.LogisticRegression(maxIter=10, labelCol='sentiment_label', featuresCol=idf.getOutputCol())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "prescription-president",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[Your Code] to build a ML pipeline and train logistic regression model\n",
    "\n",
    "\n",
    "# Define stages of the pipeline\n",
    "pipeline_stages = [\n",
    "    tokenizer,\n",
    "    stopwords,\n",
    "    tf,\n",
    "    idf,\n",
    "    lr\n",
    "]\n",
    "\n",
    "# Create the pipeline\n",
    "lr_pipeline = Pipeline(stages=pipeline_stages)\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "lr_model = lr_pipeline.fit(reviews_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "going-shoot",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make predictions with pipeline model\n",
    "predictions = lr_model.transform(reviews_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "official-freeze",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9338446403526336\n",
      "0.9273869863834877\n"
     ]
    }
   ],
   "source": [
    "import pyspark.ml.evaluation as ev\n",
    "#model evaluation for binary classification\n",
    "evaluator = ev.BinaryClassificationEvaluator(rawPredictionCol='probability', labelCol='sentiment_label')\n",
    "print(evaluator.evaluate(predictions, {evaluator.metricName: 'areaUnderROC'}))\n",
    "print(evaluator.evaluate(predictions, {evaluator.metricName: 'areaUnderPR'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-celtic",
   "metadata": {},
   "source": [
    "The prediction performance looks acceptable. Here note that TF-IDF is a long vector (here we select top 10000 words, but still a large number), so let's try something different. As mentioned in the class, another way to model text is word embedding with the Word2Vec model. So next we create word vector and use it to predict sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "provincial-senator",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create word2vec model\n",
    "word2vec = ft.Word2Vec(vectorSize=100, minCount=5, inputCol=stopwords.getOutputCol(), outputCol=\"review_word2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "possible-explorer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#same logistic regression model, but take output from word2vec model\n",
    "#[Your Code] to create a logistic regression model and build pipeline with word2vec to train logistic regession; then make predictions and evaluate model (areaUnderROC and areaUnderPR)\n",
    "\n",
    "# Logistic Regression model\n",
    "lr = cl.LogisticRegression(maxIter=10, labelCol='sentiment_label', featuresCol=\"review_word2vec\")\n",
    "\n",
    "# Define stages of the pipeline\n",
    "pipeline_stages = [\n",
    "    tokenizer,\n",
    "    stopwords,\n",
    "    word2vec,\n",
    "    lr\n",
    "]\n",
    "\n",
    "# Create the pipeline\n",
    "lr_word2vec_pipeline = Pipeline(stages=pipeline_stages)\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "lr_word2vec_model = lr_word2vec_pipeline.fit(reviews_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = lr_word2vec_model.transform(reviews_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "21f1be78-8900-4dad-99d9-b7448f9d6b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.934560428771788\n",
      "0.9316361736134724\n"
     ]
    }
   ],
   "source": [
    "#import pyspark.ml.evaluation as ev\n",
    "#model evaluation for binary classification\n",
    "#evaluator = ev.BinaryClassificationEvaluator(rawPredictionCol='probability', labelCol='sentiment_label')\n",
    "print(evaluator.evaluate(predictions, {evaluator.metricName: 'areaUnderROC'}))\n",
    "print(evaluator.evaluate(predictions, {evaluator.metricName: 'areaUnderPR'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-disposition",
   "metadata": {},
   "source": [
    "Check the prediction performance with only 100 features; word2vec model is a very useful representation of words and it reduces dimensionality significantly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-enclosure",
   "metadata": {},
   "source": [
    "In the end, let's try an alternative model. In classfication, Support Vector Machine (SVM) is commonly used and let's see how we can use it here. We will keep the orginal configuration of word2vec model (vector size is 100) here for SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "challenging-hepatitis",
   "metadata": {},
   "outputs": [],
   "source": [
    "#same word2vec model configuration is adopted here\n",
    "word2vec = ft.Word2Vec(vectorSize=100, minCount=5, inputCol=stopwords.getOutputCol(), outputCol=\"review_word2vec\")\n",
    "#create svm with LinearSVC, with features from word2vec model outputCol\n",
    "svm = cl.LinearSVC(maxIter=10, labelCol='sentiment_label', featuresCol=word2vec.getOutputCol())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "toxic-shepherd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the ml pipeline and train the model; then make predictions \n",
    "#[Your Code] to build the ML pipeline to train SVM model; then make predictions (no need to evaluate, the evaluation is slightly different here so provided below)\n",
    " \n",
    "# Define stages of the pipeline\n",
    "pipeline_stages = [\n",
    "    tokenizer,\n",
    "    stopwords,\n",
    "    word2vec,\n",
    "    svm\n",
    "]\n",
    "\n",
    "# Create the pipeline\n",
    "svm_pipeline = Pipeline(stages=pipeline_stages)\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "svm_model = svm_pipeline.fit(reviews_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = svm_model.transform(reviews_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "52028075-bd61-4ddf-b127-3a62c3e11617",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model for task 3\n",
    "from pyspark.ml import PipelineModel\n",
    "model_path = './svm_model'\n",
    "svm_model.write().overwrite().save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "endangered-yacht",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9340058906030851\n",
      "0.9311235672362355\n"
     ]
    }
   ],
   "source": [
    "#model evaluation, here slightly different for LinearSVC\n",
    "evaluator = ev.BinaryClassificationEvaluator(rawPredictionCol='rawPrediction', labelCol='sentiment_label')\n",
    "print(evaluator.evaluate(predictions, {evaluator.metricName: 'areaUnderROC'}))\n",
    "print(evaluator.evaluate(predictions, {evaluator.metricName: 'areaUnderPR'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173b514e-9e8a-4ca5-8cc0-19a06cdc2492",
   "metadata": {},
   "source": [
    "### Task 3 - Combine Spark ML and Streaming Analysis\n",
    "Now we have our sentiment prediction model with acceptable predictive performance. The last task is to combine this machine learning mode with spark streaming. That is, with a data stream, we will use the trained model to make real-time predictions. We will still use IMDB reviews and here we will create a simulated data stream from files in a directory, then receive the review stream and predict sentiment using spark structured streaming."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121cc09c-14e4-4af9-a1d6-d52d222fa1a3",
   "metadata": {},
   "source": [
    "We will first use `streaming_producer.ipynb` specifically for this assignment so that we can simulate a data stream to send files incrementally into a directory and we can read stream from the same directory. To do that, we use `streaming_producer.ipynb` - it will send random reviews in the form of csv files into `review_stream` directory. Then this spark application will read review stream data from `review_stream` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "blank-officer",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_test_stream = reviews_test.select('review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "interracial-reception",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#here we save the data as csv to allow streaming_producer.ipynb to take random reviews from it\n",
    "df = reviews_test_stream.toPandas()\n",
    "df.to_csv('review_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9a8c1f5f-f47c-4c7b-9b4a-b20413e4604b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the saved model\n",
    "from pyspark.ml import PipelineModel\n",
    "model_path = './svm_model'\n",
    "svm_model = PipelineModel.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41997310-557e-43b3-a0b3-7c8c5632e61d",
   "metadata": {},
   "source": [
    "Now we can read from the stream. For convenience, we just take the existing spark dataframe to reuse the schema. Please note the first comment for streaming producer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cdb5614a-85f0-41d8-8ded-a970212fa84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run streaming_producer.ipynb notebook (but make sure you try to work on the code below first), then go back here to run subsequent code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "assured-symbol",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read from data stream in a folder\n",
    "streaming_path = './review_stream'\n",
    "if not os.path.exists(streaming_path):\n",
    "    os.makedirs(streaming_path)\n",
    "streaming_review = spark.readStream.schema(reviews_test_stream.schema).option(\"maxFilesPerTrigger\", 1).csv(streaming_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8643e54c-4e77-4259-9287-781c877a432f",
   "metadata": {},
   "source": [
    "Here from the data stream, we want to know three results. First, how many positive or negative reviews we have received in real time? Second, how many positive or negative reviews in each time window of 60 seconds? Third, we are interested in the positive and negative reviews in each time window of 60 seconds, but with sliding window for every 30 seconds. So we will do some calculations, and to capture time window, we will use the current timestamp to create 'processing_time' (the time we receive the data) and apply window on this timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "binary-kinase",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_review_time = streaming_review.withColumn('processing_time', fn.current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4764a806-59f1-48fa-b56b-c00638f3325f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the streaming data\n",
    "streaming_sentiment = svm_model.transform(streaming_review_time)\n",
    "\n",
    "# Define the query to write the streaming data with predictions to a sink\n",
    "query = streaming_sentiment.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "# Await termination of the query\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1649ffa4-8a01-4e91-814c-a320be3c581b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a predicted text label from 'predicted column' is the prediction\n",
    "streaming_sentiment = streaming_sentiment.withColumn('predicted', fn.when(fn.col('prediction')==1.0, 'positive').otherwise('negative'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "stainless-institution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- predicted: string (nullable = false)\n",
      " |-- total_count: long (nullable = false)\n",
      "\n",
      "root\n",
      " |-- window: struct (nullable = false)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- predicted: string (nullable = false)\n",
      " |-- window_count: long (nullable = false)\n",
      "\n",
      "root\n",
      " |-- window: struct (nullable = true)\n",
      " |    |-- start: timestamp (nullable = true)\n",
      " |    |-- end: timestamp (nullable = true)\n",
      " |-- predicted: string (nullable = false)\n",
      " |-- sliding_window_count: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#[Your Code] to do transformations to get the results we need\n",
    "#first, get total number of positive and negative reviews we have received\n",
    "#second, still number of positive and negative reviews we received, but by time window (60 seconds)\n",
    "#third, still number of positive and negative reviews we received, but by sliding time window (60 seconds window for every 30 seconds)\n",
    "#names of streaming dataframes are 'sentiment_count', 'sentiment_window_count', 'sentiment_sliding_window_count' (see the next cell)\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import window\n",
    "\n",
    "# First transformation: Total number of positive and negative reviews\n",
    "sentiment_count = streaming_sentiment.groupBy('predicted').count()\n",
    "\n",
    "# Second transformation: Number of positive and negative reviews by time window (60 seconds)\n",
    "sentiment_window_count = streaming_sentiment.groupBy(window(\"processing_time\", \"60 seconds\"), \"predicted\").count()\n",
    "\n",
    "# Third transformation: Number of positive and negative reviews by sliding time window (60 seconds window for every 30 seconds)\n",
    "sentiment_sliding_window_count = streaming_sentiment.groupBy(window(\"processing_time\", \"60 seconds\", \"30 seconds\"), \"predicted\").count()\n",
    "\n",
    "# Naming the streaming dataframes\n",
    "sentiment_count = sentiment_count.withColumnRenamed(\"count\", \"total_count\")\n",
    "sentiment_window_count = sentiment_window_count.withColumnRenamed(\"count\", \"window_count\")\n",
    "sentiment_sliding_window_count = sentiment_sliding_window_count.withColumnRenamed(\"count\", \"sliding_window_count\")\n",
    "\n",
    "# Printing the schema of the dataframes to ensure correctness\n",
    "sentiment_count.printSchema()\n",
    "sentiment_window_count.printSchema()\n",
    "sentiment_sliding_window_count.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "marked-religion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#now we have two streaming dataframe results\n",
    "print(sentiment_count.isStreaming)\n",
    "print(sentiment_window_count.isStreaming)\n",
    "print(sentiment_sliding_window_count.isStreaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cd4a45b2-f518-4e25-8817-2d05f577714a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master('local[4]') \\\n",
    "                    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "                    .config(\"spark.jars\", \"graphframes-0.8.3-spark3.5-s_2.12.jar\") \\\n",
    "                    .config(\"spark.packages\", \"graphframes:graphframes:0.8.3-spark3.5-s_2.12\") \\\n",
    "                    .appName('spark_graphframe').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aging-proxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#[Your Code] to define the query variables and set the result tables\n",
    "#name of quiery variables are 'query_sentiment', 'query_sentiment_window', 'query_sentiment_sliding_window' (see the last cell)\n",
    "#name of result tables are 'sentiment', 'sentiment_window', 'sentiment_sliding_window' (see the next cell)\n",
    "\n",
    "# Define query variables\n",
    "query_sentiment = sentiment_count \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"sentiment\") \\\n",
    "    .start()\n",
    "\n",
    "query_sentiment_window = sentiment_window_count \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"sentiment_window\") \\\n",
    "    .start()\n",
    "\n",
    "query_sentiment_sliding_window = sentiment_sliding_window_count \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"sentiment_sliding_window\") \\\n",
    "    .start()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "df2323bc-b71d-495c-bbf0-84aa51f7b281",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|predicted|total_count|\n",
      "+---------+-----------+\n",
      "| positive|         26|\n",
      "| negative|         29|\n",
      "+---------+-----------+\n",
      "\n",
      "+------------------------------------------+---------+------------+\n",
      "|window                                    |predicted|window_count|\n",
      "+------------------------------------------+---------+------------+\n",
      "|{2024-03-17 04:43:00, 2024-03-17 04:44:00}|positive |26          |\n",
      "|{2024-03-17 04:43:00, 2024-03-17 04:44:00}|negative |29          |\n",
      "+------------------------------------------+---------+------------+\n",
      "\n",
      "+------------------------------------------+---------+--------------------+\n",
      "|window                                    |predicted|sliding_window_count|\n",
      "+------------------------------------------+---------+--------------------+\n",
      "|{2024-03-17 04:43:00, 2024-03-17 04:44:00}|negative |29                  |\n",
      "|{2024-03-17 04:43:00, 2024-03-17 04:44:00}|positive |26                  |\n",
      "|{2024-03-17 04:43:30, 2024-03-17 04:44:30}|positive |26                  |\n",
      "|{2024-03-17 04:43:30, 2024-03-17 04:44:30}|negative |29                  |\n",
      "+------------------------------------------+---------+--------------------+\n",
      "\n",
      "+---------+-----------+\n",
      "|predicted|total_count|\n",
      "+---------+-----------+\n",
      "| positive|         26|\n",
      "| negative|         29|\n",
      "+---------+-----------+\n",
      "\n",
      "+------------------------------------------+---------+------------+\n",
      "|window                                    |predicted|window_count|\n",
      "+------------------------------------------+---------+------------+\n",
      "|{2024-03-17 04:43:00, 2024-03-17 04:44:00}|positive |26          |\n",
      "|{2024-03-17 04:43:00, 2024-03-17 04:44:00}|negative |29          |\n",
      "+------------------------------------------+---------+------------+\n",
      "\n",
      "+------------------------------------------+---------+--------------------+\n",
      "|window                                    |predicted|sliding_window_count|\n",
      "+------------------------------------------+---------+--------------------+\n",
      "|{2024-03-17 04:43:00, 2024-03-17 04:44:00}|positive |26                  |\n",
      "|{2024-03-17 04:43:00, 2024-03-17 04:44:00}|negative |29                  |\n",
      "|{2024-03-17 04:43:30, 2024-03-17 04:44:30}|negative |29                  |\n",
      "|{2024-03-17 04:43:30, 2024-03-17 04:44:30}|positive |26                  |\n",
      "+------------------------------------------+---------+--------------------+\n",
      "\n",
      "+---------+-----------+\n",
      "|predicted|total_count|\n",
      "+---------+-----------+\n",
      "| positive|         26|\n",
      "| negative|         29|\n",
      "+---------+-----------+\n",
      "\n",
      "+------------------------------------------+---------+------------+\n",
      "|window                                    |predicted|window_count|\n",
      "+------------------------------------------+---------+------------+\n",
      "|{2024-03-17 04:43:00, 2024-03-17 04:44:00}|negative |29          |\n",
      "|{2024-03-17 04:43:00, 2024-03-17 04:44:00}|positive |26          |\n",
      "+------------------------------------------+---------+------------+\n",
      "\n",
      "+------------------------------------------+---------+--------------------+\n",
      "|window                                    |predicted|sliding_window_count|\n",
      "+------------------------------------------+---------+--------------------+\n",
      "|{2024-03-17 04:43:00, 2024-03-17 04:44:00}|positive |26                  |\n",
      "|{2024-03-17 04:43:00, 2024-03-17 04:44:00}|negative |29                  |\n",
      "|{2024-03-17 04:43:30, 2024-03-17 04:44:30}|negative |29                  |\n",
      "|{2024-03-17 04:43:30, 2024-03-17 04:44:30}|positive |26                  |\n",
      "+------------------------------------------+---------+--------------------+\n",
      "\n",
      "+---------+-----------+\n",
      "|predicted|total_count|\n",
      "+---------+-----------+\n",
      "| positive|         26|\n",
      "| negative|         29|\n",
      "+---------+-----------+\n",
      "\n",
      "+------------------------------------------+---------+------------+\n",
      "|window                                    |predicted|window_count|\n",
      "+------------------------------------------+---------+------------+\n",
      "|{2024-03-17 04:43:00, 2024-03-17 04:44:00}|positive |26          |\n",
      "|{2024-03-17 04:43:00, 2024-03-17 04:44:00}|negative |29          |\n",
      "+------------------------------------------+---------+------------+\n",
      "\n",
      "+------------------------------------------+---------+--------------------+\n",
      "|window                                    |predicted|sliding_window_count|\n",
      "+------------------------------------------+---------+--------------------+\n",
      "|{2024-03-17 04:43:00, 2024-03-17 04:44:00}|negative |29                  |\n",
      "|{2024-03-17 04:43:00, 2024-03-17 04:44:00}|positive |26                  |\n",
      "|{2024-03-17 04:43:30, 2024-03-17 04:44:30}|negative |29                  |\n",
      "|{2024-03-17 04:43:30, 2024-03-17 04:44:30}|positive |26                  |\n",
      "+------------------------------------------+---------+--------------------+\n",
      "\n",
      "+---------+-----------+\n",
      "|predicted|total_count|\n",
      "+---------+-----------+\n",
      "| positive|         26|\n",
      "| negative|         29|\n",
      "+---------+-----------+\n",
      "\n",
      "+------------------------------------------+---------+------------+\n",
      "|window                                    |predicted|window_count|\n",
      "+------------------------------------------+---------+------------+\n",
      "|{2024-03-17 04:43:00, 2024-03-17 04:44:00}|negative |29          |\n",
      "|{2024-03-17 04:43:00, 2024-03-17 04:44:00}|positive |26          |\n",
      "+------------------------------------------+---------+------------+\n",
      "\n",
      "+------------------------------------------+---------+--------------------+\n",
      "|window                                    |predicted|sliding_window_count|\n",
      "+------------------------------------------+---------+--------------------+\n",
      "|{2024-03-17 04:43:00, 2024-03-17 04:44:00}|positive |26                  |\n",
      "|{2024-03-17 04:43:00, 2024-03-17 04:44:00}|negative |29                  |\n",
      "|{2024-03-17 04:43:30, 2024-03-17 04:44:30}|negative |29                  |\n",
      "|{2024-03-17 04:43:30, 2024-03-17 04:44:30}|positive |26                  |\n",
      "+------------------------------------------+---------+--------------------+\n",
      "\n",
      "+---------+-----------+\n",
      "|predicted|total_count|\n",
      "+---------+-----------+\n",
      "| positive|         26|\n",
      "| negative|         29|\n",
      "+---------+-----------+\n",
      "\n",
      "+------------------------------------------+---------+------------+\n",
      "|window                                    |predicted|window_count|\n",
      "+------------------------------------------+---------+------------+\n",
      "|{2024-03-17 04:43:00, 2024-03-17 04:44:00}|negative |29          |\n",
      "|{2024-03-17 04:43:00, 2024-03-17 04:44:00}|positive |26          |\n",
      "+------------------------------------------+---------+------------+\n",
      "\n",
      "+------------------------------------------+---------+--------------------+\n",
      "|window                                    |predicted|sliding_window_count|\n",
      "+------------------------------------------+---------+--------------------+\n",
      "|{2024-03-17 04:43:00, 2024-03-17 04:44:00}|positive |26                  |\n",
      "|{2024-03-17 04:43:00, 2024-03-17 04:44:00}|negative |29                  |\n",
      "|{2024-03-17 04:43:30, 2024-03-17 04:44:30}|positive |26                  |\n",
      "|{2024-03-17 04:43:30, 2024-03-17 04:44:30}|negative |29                  |\n",
      "+------------------------------------------+---------+--------------------+\n",
      "\n",
      "+---------+-----------+\n",
      "|predicted|total_count|\n",
      "+---------+-----------+\n",
      "| positive|         26|\n",
      "| negative|         29|\n",
      "+---------+-----------+\n",
      "\n",
      "+------------------------------------------+---------+------------+\n",
      "|window                                    |predicted|window_count|\n",
      "+------------------------------------------+---------+------------+\n",
      "|{2024-03-17 04:43:00, 2024-03-17 04:44:00}|positive |26          |\n",
      "|{2024-03-17 04:43:00, 2024-03-17 04:44:00}|negative |29          |\n",
      "+------------------------------------------+---------+------------+\n",
      "\n",
      "+------------------------------------------+---------+--------------------+\n",
      "|window                                    |predicted|sliding_window_count|\n",
      "+------------------------------------------+---------+--------------------+\n",
      "|{2024-03-17 04:43:00, 2024-03-17 04:44:00}|positive |26                  |\n",
      "|{2024-03-17 04:43:00, 2024-03-17 04:44:00}|negative |29                  |\n",
      "|{2024-03-17 04:43:30, 2024-03-17 04:44:30}|negative |29                  |\n",
      "|{2024-03-17 04:43:30, 2024-03-17 04:44:30}|positive |26                  |\n",
      "+------------------------------------------+---------+--------------------+\n",
      "\n",
      "+---------+-----------+\n",
      "|predicted|total_count|\n",
      "+---------+-----------+\n",
      "| positive|         26|\n",
      "| negative|         29|\n",
      "+---------+-----------+\n",
      "\n",
      "+------------------------------------------+---------+------------+\n",
      "|window                                    |predicted|window_count|\n",
      "+------------------------------------------+---------+------------+\n",
      "|{2024-03-17 04:43:00, 2024-03-17 04:44:00}|negative |29          |\n",
      "|{2024-03-17 04:43:00, 2024-03-17 04:44:00}|positive |26          |\n",
      "+------------------------------------------+---------+------------+\n",
      "\n",
      "+------------------------------------------+---------+--------------------+\n",
      "|window                                    |predicted|sliding_window_count|\n",
      "+------------------------------------------+---------+--------------------+\n",
      "|{2024-03-17 04:43:00, 2024-03-17 04:44:00}|positive |26                  |\n",
      "|{2024-03-17 04:43:00, 2024-03-17 04:44:00}|negative |29                  |\n",
      "|{2024-03-17 04:43:30, 2024-03-17 04:44:30}|positive |26                  |\n",
      "|{2024-03-17 04:43:30, 2024-03-17 04:44:30}|negative |29                  |\n",
      "+------------------------------------------+---------+--------------------+\n",
      "\n",
      "+---------+-----------+\n",
      "|predicted|total_count|\n",
      "+---------+-----------+\n",
      "| positive|         26|\n",
      "| negative|         29|\n",
      "+---------+-----------+\n",
      "\n",
      "+------------------------------------------+---------+------------+\n",
      "|window                                    |predicted|window_count|\n",
      "+------------------------------------------+---------+------------+\n",
      "|{2024-03-17 04:43:00, 2024-03-17 04:44:00}|negative |29          |\n",
      "|{2024-03-17 04:43:00, 2024-03-17 04:44:00}|positive |26          |\n",
      "+------------------------------------------+---------+------------+\n",
      "\n",
      "+------------------------------------------+---------+--------------------+\n",
      "|window                                    |predicted|sliding_window_count|\n",
      "+------------------------------------------+---------+--------------------+\n",
      "|{2024-03-17 04:43:00, 2024-03-17 04:44:00}|positive |26                  |\n",
      "|{2024-03-17 04:43:00, 2024-03-17 04:44:00}|negative |29                  |\n",
      "|{2024-03-17 04:43:30, 2024-03-17 04:44:30}|positive |26                  |\n",
      "|{2024-03-17 04:43:30, 2024-03-17 04:44:30}|negative |29                  |\n",
      "+------------------------------------------+---------+--------------------+\n",
      "\n",
      "+---------+-----------+\n",
      "|predicted|total_count|\n",
      "+---------+-----------+\n",
      "| positive|         26|\n",
      "| negative|         29|\n",
      "+---------+-----------+\n",
      "\n",
      "+------------------------------------------+---------+------------+\n",
      "|window                                    |predicted|window_count|\n",
      "+------------------------------------------+---------+------------+\n",
      "|{2024-03-17 04:43:00, 2024-03-17 04:44:00}|positive |26          |\n",
      "|{2024-03-17 04:43:00, 2024-03-17 04:44:00}|negative |29          |\n",
      "+------------------------------------------+---------+------------+\n",
      "\n",
      "+------------------------------------------+---------+--------------------+\n",
      "|window                                    |predicted|sliding_window_count|\n",
      "+------------------------------------------+---------+--------------------+\n",
      "|{2024-03-17 04:43:00, 2024-03-17 04:44:00}|positive |26                  |\n",
      "|{2024-03-17 04:43:00, 2024-03-17 04:44:00}|negative |29                  |\n",
      "|{2024-03-17 04:43:30, 2024-03-17 04:44:30}|positive |26                  |\n",
      "|{2024-03-17 04:43:30, 2024-03-17 04:44:30}|negative |29                  |\n",
      "+------------------------------------------+---------+--------------------+\n",
      "\n",
      "+---------+-----------+\n",
      "|predicted|total_count|\n",
      "+---------+-----------+\n",
      "| positive|         26|\n",
      "| negative|         29|\n",
      "+---------+-----------+\n",
      "\n",
      "+------------------------------------------+---------+------------+\n",
      "|window                                    |predicted|window_count|\n",
      "+------------------------------------------+---------+------------+\n",
      "|{2024-03-17 04:43:00, 2024-03-17 04:44:00}|negative |29          |\n",
      "|{2024-03-17 04:43:00, 2024-03-17 04:44:00}|positive |26          |\n",
      "+------------------------------------------+---------+------------+\n",
      "\n",
      "+------------------------------------------+---------+--------------------+\n",
      "|window                                    |predicted|sliding_window_count|\n",
      "+------------------------------------------+---------+--------------------+\n",
      "|{2024-03-17 04:43:00, 2024-03-17 04:44:00}|negative |29                  |\n",
      "|{2024-03-17 04:43:00, 2024-03-17 04:44:00}|positive |26                  |\n",
      "|{2024-03-17 04:43:30, 2024-03-17 04:44:30}|positive |26                  |\n",
      "|{2024-03-17 04:43:30, 2024-03-17 04:44:30}|negative |29                  |\n",
      "+------------------------------------------+---------+--------------------+\n",
      "\n",
      "+---------+-----------+\n",
      "|predicted|total_count|\n",
      "+---------+-----------+\n",
      "| positive|         26|\n",
      "| negative|         29|\n",
      "+---------+-----------+\n",
      "\n",
      "+------------------------------------------+---------+------------+\n",
      "|window                                    |predicted|window_count|\n",
      "+------------------------------------------+---------+------------+\n",
      "|{2024-03-17 04:43:00, 2024-03-17 04:44:00}|positive |26          |\n",
      "|{2024-03-17 04:43:00, 2024-03-17 04:44:00}|negative |29          |\n",
      "+------------------------------------------+---------+------------+\n",
      "\n",
      "+------------------------------------------+---------+--------------------+\n",
      "|window                                    |predicted|sliding_window_count|\n",
      "+------------------------------------------+---------+--------------------+\n",
      "|{2024-03-17 04:43:00, 2024-03-17 04:44:00}|positive |26                  |\n",
      "|{2024-03-17 04:43:00, 2024-03-17 04:44:00}|negative |29                  |\n",
      "|{2024-03-17 04:43:30, 2024-03-17 04:44:30}|positive |26                  |\n",
      "|{2024-03-17 04:43:30, 2024-03-17 04:44:30}|negative |29                  |\n",
      "+------------------------------------------+---------+--------------------+\n",
      "\n",
      "+---------+-----------+\n",
      "|predicted|total_count|\n",
      "+---------+-----------+\n",
      "| positive|         26|\n",
      "| negative|         29|\n",
      "+---------+-----------+\n",
      "\n",
      "+------------------------------------------+---------+------------+\n",
      "|window                                    |predicted|window_count|\n",
      "+------------------------------------------+---------+------------+\n",
      "|{2024-03-17 04:43:00, 2024-03-17 04:44:00}|positive |26          |\n",
      "|{2024-03-17 04:43:00, 2024-03-17 04:44:00}|negative |29          |\n",
      "+------------------------------------------+---------+------------+\n",
      "\n",
      "+------------------------------------------+---------+--------------------+\n",
      "|window                                    |predicted|sliding_window_count|\n",
      "+------------------------------------------+---------+--------------------+\n",
      "|{2024-03-17 04:43:00, 2024-03-17 04:44:00}|negative |29                  |\n",
      "|{2024-03-17 04:43:00, 2024-03-17 04:44:00}|positive |26                  |\n",
      "|{2024-03-17 04:43:30, 2024-03-17 04:44:30}|negative |29                  |\n",
      "|{2024-03-17 04:43:30, 2024-03-17 04:44:30}|positive |26                  |\n",
      "+------------------------------------------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create a dashboard for 2 minutes to show the live results from the three result tbales\n",
    "import time\n",
    "start_time = time.time()\n",
    "current_time = time.time()\n",
    "while current_time - start_time <= 120:\n",
    "    spark.sql('select * from sentiment').show()\n",
    "    spark.sql('select * from sentiment_window order by window').show(truncate=False)\n",
    "    spark.sql('select * from sentiment_sliding_window order by window').show(truncate=False)\n",
    "    current_time = time.time()\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "parallel-daisy",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stop query to finish streaming analysis\n",
    "query_sentiment.stop()\n",
    "query_sentiment_window.stop()\n",
    "query_sentiment_sliding_window.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05782923-a0b4-4365-a340-8b7d4bd8022b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###---------Code End-----------###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
